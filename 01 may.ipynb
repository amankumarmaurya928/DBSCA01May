{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the total\\n   number of target classes. The matrix compares the actual target values with those predicted by the machine learning model.\\n   A confusion matrix is a table that allows you to visualize the performance of a classification model. You can also use the \\n   information in it to calculate measures that can help you determine the usefulness of the model. Rows represent predicted \\n   classifications, while columns represent the true classes from the data.\\n   Metrics like accuracy, precision, recall are good ways to evaluate classification models for balanced datasets, but if the\\n   data is imbalanced then other methods like ROC/AUC perform better in evaluating the model performance.\\n   A confusion matrix, also known as an error matrix, is a summarized table used to assess the performance of a classification\\n   model. The number of correct and incorrect predictions are summarized with count values and broken down by each class.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "'''A Confusion matrix is an N x N matrix used for evaluating the performance of a classification model, where N is the total\n",
    "   number of target classes. The matrix compares the actual target values with those predicted by the machine learning model.\n",
    "   A confusion matrix is a table that allows you to visualize the performance of a classification model. You can also use the \n",
    "   information in it to calculate measures that can help you determine the usefulness of the model. Rows represent predicted \n",
    "   classifications, while columns represent the true classes from the data.\n",
    "   Metrics like accuracy, precision, recall are good ways to evaluate classification models for balanced datasets, but if the\n",
    "   data is imbalanced then other methods like ROC/AUC perform better in evaluating the model performance.\n",
    "   A confusion matrix, also known as an error matrix, is a summarized table used to assess the performance of a classification\n",
    "   model. The number of correct and incorrect predictions are summarized with count values and broken down by each class.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Confusion Matrix is a classification matrix used in supervised learning. Confusion matrix is helpful when one wants to\\n   evaluate the performance of a classifier or Machine Learning algorithm. It gives a good comparison of the predicted and \\n   real/actual values.\\n   A confusion matrix is a specific table layout that allows visualization of the performance of an algorithm, typically a\\n   supervised learning. A correlation matrix is a table showing correlation coefficients between variables.\\n   A confusion matrix is a table that allows you to visualize the performance of a classification model. You can also use the\\n   information in it to calculate measures that can help you determine the usefulness of the model. Rows represent predicted\\n   classifications, while columns represent the true classes from the data.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "'''Confusion Matrix is a classification matrix used in supervised learning. Confusion matrix is helpful when one wants to\n",
    "   evaluate the performance of a classifier or Machine Learning algorithm. It gives a good comparison of the predicted and \n",
    "   real/actual values.\n",
    "   A confusion matrix is a specific table layout that allows visualization of the performance of an algorithm, typically a\n",
    "   supervised learning. A correlation matrix is a table showing correlation coefficients between variables.\n",
    "   A confusion matrix is a table that allows you to visualize the performance of a classification model. You can also use the\n",
    "   information in it to calculate measures that can help you determine the usefulness of the model. Rows represent predicted\n",
    "   classifications, while columns represent the true classes from the data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extrinsic evaluation is the best way to evaluate the performance of a language model by embedding it in an application and\\n   measuring how much the application improves. It is an end-to-end evaluation where we can understand if a particular \\n   improvement in a component is really going to help the task at hand.\\n   NLP “accuracy” (often termed NLP performance) is measured using two specific performance metrics: Precision and Recall. If\\n   these names sound strange, it might be because they come from the information retrieval and computer science community.\\n   Traditionally, language model performance is measured by perplexity, cross entropy, and bits-per-character (BPC). As \\n   language models are increasingly being used as pre-trained models for other NLP tasks, they are often also evaluated based \\n   on how well they perform on downstream tasks.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3\n",
    "'''Extrinsic evaluation is the best way to evaluate the performance of a language model by embedding it in an application and\n",
    "   measuring how much the application improves. It is an end-to-end evaluation where we can understand if a particular \n",
    "   improvement in a component is really going to help the task at hand.\n",
    "   NLP “accuracy” (often termed NLP performance) is measured using two specific performance metrics: Precision and Recall. If\n",
    "   these names sound strange, it might be because they come from the information retrieval and computer science community.\n",
    "   Traditionally, language model performance is measured by perplexity, cross entropy, and bits-per-character (BPC). As \n",
    "   language models are increasingly being used as pre-trained models for other NLP tasks, they are often also evaluated based \n",
    "   on how well they perform on downstream tasks.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In an intrinsic evaluation, quality of NLP systems outputs is evaluated against pre-determined ground truth (reference text)\\n   whereas an extrinsic evaluation is aimed at evaluating systems outputs based on their impact on the performance of other\\n   NLP systems.\\n   There are two types of evaluation metrics for clustering, Extrinsic Measures: These measures require ground truth labels,\\n   which may not be available in practice. Intrinsic Measures: These measures do not require ground truth labels \\n   (applicable to all unsupervised learning results).\\n   Extrinsic methods assess the impact of a summarization system on specific information-seeking task performance based on\\n   measures such as success rate, time-to-completion, and decision-making accuracy.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "'''In an intrinsic evaluation, quality of NLP systems outputs is evaluated against pre-determined ground truth (reference text)\n",
    "   whereas an extrinsic evaluation is aimed at evaluating systems outputs based on their impact on the performance of other\n",
    "   NLP systems.\n",
    "   There are two types of evaluation metrics for clustering, Extrinsic Measures: These measures require ground truth labels,\n",
    "   which may not be available in practice. Intrinsic Measures: These measures do not require ground truth labels \n",
    "   (applicable to all unsupervised learning results).\n",
    "   Extrinsic methods assess the impact of a summarization system on specific information-seeking task performance based on\n",
    "   measures such as success rate, time-to-completion, and decision-making accuracy.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Need for Confusion Matrix in Machine learning\\n   It evaluates the performance of the classification models, when they make predictions on test data, and tells how good our\\n   classification model is. It not only tells the error made by the classifiers but also the type of errors such as it is\\n   either type-I or type-II error.\\n   A confusion matrix represents the prediction summary in matrix form. It shows how many prediction are correct and incorrect\\n   per class. It helps in understanding the classes that are being confused by model as other class.\\n   A confusion matrix is a table that allows you to visualize the performance of a classification model. You can also use the \\n   information in it to calculate measures that can help you determine the usefulness of the model. Rows represent predicted \\n   classifications, while columns represent the true classes from the data.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "'''Need for Confusion Matrix in Machine learning\n",
    "   It evaluates the performance of the classification models, when they make predictions on test data, and tells how good our\n",
    "   classification model is. It not only tells the error made by the classifiers but also the type of errors such as it is\n",
    "   either type-I or type-II error.\n",
    "   A confusion matrix represents the prediction summary in matrix form. It shows how many prediction are correct and incorrect\n",
    "   per class. It helps in understanding the classes that are being confused by model as other class.\n",
    "   A confusion matrix is a table that allows you to visualize the performance of a classification model. You can also use the \n",
    "   information in it to calculate measures that can help you determine the usefulness of the model. Rows represent predicted \n",
    "   classifications, while columns represent the true classes from the data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In general, intrinsic methods evaluate a clustering by examining how well the clusters are separated and how compact the\\n   clusters are. Many intrinsic methods have the advantage of a similarity metric between objects in the data set.\\n   Intrinsic evaluation can be further divided into two main categories: adequacy and fluency.\\n   There are two types of evaluation metrics for clustering, Extrinsic Measures: These measures require ground truth labels,\\n   which may not be available in practice. Intrinsic Measures: These measures do not require ground truth labels (applicable\\n   to all unsupervised learning results).\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "'''In general, intrinsic methods evaluate a clustering by examining how well the clusters are separated and how compact the\n",
    "   clusters are. Many intrinsic methods have the advantage of a similarity metric between objects in the data set.\n",
    "   Intrinsic evaluation can be further divided into two main categories: adequacy and fluency.\n",
    "   There are two types of evaluation metrics for clustering, Extrinsic Measures: These measures require ground truth labels,\n",
    "   which may not be available in practice. Intrinsic Measures: These measures do not require ground truth labels (applicable\n",
    "   to all unsupervised learning results).\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In general, the main disadvantage of accuracy is that it masks the issue of class imbalance. For example if the data \\n   contains only 10% of positive instances, a majority baseline classifier which always assigns the negative label would reach\\n   90% accuracy since it would correctly predict 90% instances.\\n   So, accuracy does not holds good for imbalanced data. In business scenarios, most data won't be balanced and so accuracy\\n   becomes poor measure of evaluation for our classification model.\\n   Accuracy and error rate is the standard measures for characterising classification model performance. Because practitioners\\n   built intuitions on datasets with an equal class distribution, classification accuracy fails on classification tasks with a\\n   skewed class distribution.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7\n",
    "'''In general, the main disadvantage of accuracy is that it masks the issue of class imbalance. For example if the data \n",
    "   contains only 10% of positive instances, a majority baseline classifier which always assigns the negative label would reach\n",
    "   90% accuracy since it would correctly predict 90% instances.\n",
    "   So, accuracy does not holds good for imbalanced data. In business scenarios, most data won't be balanced and so accuracy\n",
    "   becomes poor measure of evaluation for our classification model.\n",
    "   Accuracy and error rate is the standard measures for characterising classification model performance. Because practitioners\n",
    "   built intuitions on datasets with an equal class distribution, classification accuracy fails on classification tasks with a\n",
    "   skewed class distribution.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
